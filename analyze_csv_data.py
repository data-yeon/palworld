#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import json
from collections import Counter

def analyze_pal_database():
    """íŒ° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬"""
    
    # CSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv('perfect_complete_pal_database_214.csv')
    
    print("=" * 80)
    print("ğŸ¯ íŒ°ì›”ë“œ ì™„ë²½ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ")
    print("=" * 80)
    
    # 1. ê¸°ë³¸ í†µê³„
    print("\nğŸ“Š **ê¸°ë³¸ í†µê³„ ì •ë³´**")
    print(f"ì´ íŒ° ìˆ˜: {len(df)}ê°œ")
    print(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
    
    # 2. ID ë²”ìœ„ ë¶„ì„
    regular_pals = df[~df['id'].astype(str).str.contains('B|S', na=False)]
    b_variants = df[df['id'].astype(str).str.contains('B', na=False)]
    s_series = df[df['id'].astype(str).str.contains('S', na=False)]
    
    print(f"\nğŸ”¢ **ID ë¶„í¬**")
    print(f"ì¼ë°˜ íŒ°: {len(regular_pals)}ê°œ")
    print(f"B ë³€í˜•: {len(b_variants)}ê°œ")
    print(f"S ì‹œë¦¬ì¦ˆ: {len(s_series)}ê°œ")
    
    # 3. íƒ€ì… ë¶„ì„
    print(f"\nğŸŒŸ **íƒ€ì…ë³„ ë¶„í¬**")
    type1_counts = Counter(df['type1'].dropna())
    type2_counts = Counter(df['type2'].dropna())
    
    print("ì£¼ ì†ì„±:")
    for type_name, count in type1_counts.most_common():
        print(f"  {type_name}: {count}ê°œ")
    
    print("\në¶€ ì†ì„±:")
    for type_name, count in type2_counts.most_common():
        print(f"  {type_name}: {count}ê°œ")
    
    # 4. í¬ê·€ë„ ë¶„ì„
    print(f"\nâ­ **í¬ê·€ë„ë³„ ë¶„í¬**")
    rarity_counts = Counter(df['rarity'].dropna())
    for rarity, count in sorted(rarity_counts.items()):
        print(f"í¬ê·€ë„ {rarity}: {count}ê°œ")
    
    # 5. í¬ê¸°ë³„ ë¶„í¬
    print(f"\nğŸ“ **í¬ê¸°ë³„ ë¶„í¬**")
    size_counts = Counter(df['size'].dropna())
    for size, count in size_counts.most_common():
        print(f"  {size}: {count}ê°œ")
    
    # 6. ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ)
    print(f"\nğŸ“‹ **ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ íŒ°)**")
    print("-" * 80)
    for idx, row in df.head(10).iterrows():
        print(f"ID: {row['id']} | {row['name']} ({row['englishName']})")
        print(f"   íƒ€ì…: {row['type1']}" + (f"/{row['type2']}" if pd.notna(row['type2']) else ""))
        print(f"   ìŠ¤íƒ¯: HP{row['hp']} ATK{row['attack']} DEF{row['defense']}")
        print(f"   ì„¤ëª…: {row['description'][:50]}..." if len(str(row['description'])) > 50 else f"   ì„¤ëª…: {row['description']}")
        print()
    
    # 7. B ë³€í˜• íŒ°ë“¤
    if len(b_variants) > 0:
        print(f"\nğŸŒˆ **B ë³€í˜• íŒ°ë“¤ ({len(b_variants)}ê°œ)**")
        print("-" * 50)
        for idx, row in b_variants.iterrows():
            print(f"ID: {row['id']} | {row['name']} ({row['englishName']})")
    
    # 8. S ì‹œë¦¬ì¦ˆ íŒ°ë“¤
    if len(s_series) > 0:
        print(f"\nğŸ‘» **S ì‹œë¦¬ì¦ˆ íŒ°ë“¤ ({len(s_series)}ê°œ)**")
        print("-" * 50)
        for idx, row in s_series.iterrows():
            print(f"ID: {row['id']} | {row['name']} ({row['englishName']})")
    
    # 9. ìµœì‹  í¬ë¡¤ë§ëœ íŒ°ë“¤ (116ë²ˆ ì´í›„)
    latest_pals = df[df['id'].astype(str).str.match(r'^(11[6-9]|1[2-5][0-9])$', na=False)]
    if len(latest_pals) > 0:
        print(f"\nğŸ†• **ìµœì‹  í¬ë¡¤ë§ëœ íŒ°ë“¤ (116ë²ˆ ì´í›„, {len(latest_pals)}ê°œ)**")
        print("-" * 60)
        for idx, row in latest_pals.iterrows():
            print(f"ID: {row['id']} | {row['name']} ({row['englishName']}) - {row['type1']}" + 
                  (f"/{row['type2']}" if pd.notna(row['type2']) else ""))
    
    # 10. íŒŒì¼ ì •ë³´
    print(f"\nğŸ’¾ **íŒŒì¼ ì •ë³´**")
    print(f"íŒŒì¼ëª…: perfect_complete_pal_database_214.csv")
    print(f"ì»¬ëŸ¼: {', '.join(df.columns.tolist())}")
    
    # 11. ë°ì´í„° í’ˆì§ˆ ì²´í¬
    print(f"\nğŸ” **ë°ì´í„° í’ˆì§ˆ ì²´í¬**")
    print(f"ë¹ˆ ì´ë¦„: {df['name'].isnull().sum()}ê°œ")
    print(f"ë¹ˆ ì˜ì–´ëª…: {df['englishName'].isnull().sum()}ê°œ")
    print(f"ë¹ˆ ì„¤ëª…: {df['description'].isnull().sum()}ê°œ")
    print(f"ë¹ˆ íƒ€ì…1: {df['type1'].isnull().sum()}ê°œ")
    
    # 12. CSV ë¯¸ë¦¬ë³´ê¸° ì €ì¥
    preview_data = []
    for idx, row in df.head(20).iterrows():
        preview_data.append({
            'ID': row['id'],
            'ì´ë¦„': row['name'],
            'ì˜ì–´ëª…': row['englishName'],
            'íƒ€ì…': f"{row['type1']}" + (f"/{row['type2']}" if pd.notna(row['type2']) else ""),
            'HP': row['hp'],
            'ê³µê²©': row['attack'],
            'ë°©ì–´': row['defense'],
            'í¬ê·€ë„': row['rarity'],
            'í¬ê¸°': row['size']
        })
    
    # JSONìœ¼ë¡œ ë¯¸ë¦¬ë³´ê¸° ì €ì¥
    with open('csv_preview_analysis.json', 'w', encoding='utf-8') as f:
        json.dump({
            'total_count': len(df),
            'regular_pals': len(regular_pals),
            'b_variants': len(b_variants),
            's_series': len(s_series),
            'type1_distribution': dict(type1_counts),
            'type2_distribution': dict(type2_counts),
            'rarity_distribution': dict(rarity_counts),
            'size_distribution': dict(size_counts),
            'preview_data': preview_data,
            'columns': df.columns.tolist()
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ë¯¸ë¦¬ë³´ê¸° ë°ì´í„°ê°€ 'csv_preview_analysis.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 80)

if __name__ == "__main__":
    analyze_pal_database() 