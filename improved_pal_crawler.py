#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ íŒ°ì›”ë“œ í¬ë¡¤ëŸ¬
ìš°ì„ ìˆœìœ„: passiveSkills, Active Skills ìƒì„¸, B variants ëŒ€ëŸ‰ ì¶”ê°€
"""

import requests
import csv
import json
import time
import re
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class ImprovedPalCrawler:
    def __init__(self):
        self.base_url = "https://paldb.cc"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ì•Œë ¤ì§„ B variants ë¦¬ìŠ¤íŠ¸ (ê²€ì¦ ê²°ê³¼ì—ì„œ í™•ì¸ëœ 76ê°œ)
        self.known_b_variants = [
            "5B", "6B", "10B", "11B", "12B", "13B", "14B", "15B", "16B", "17B",
            "24B", "26B", "27B", "28B", "29B", "30B", "31B", "34B", "36B", "38B",
            "39B", "42B", "43B", "44B", "45B", "46B", "47B", "51B", "52B", "55B",
            "56B", "57B", "59B", "60B", "61B", "64B", "65B", "67B", "69B", "71B",
            "72B", "75B", "76B", "77B", "78B", "80B", "82B", "83B", "84B", "85B",
            "86B", "87B", "88B", "89B", "90B", "91B", "92B", "95B", "96B", "98B",
            "100B", "101B", "102B", "103B", "104B", "105B", "106B", "107B", "108B",
            "109B", "110B", "111B", "112B", "113B", "114B", "115B"
        ]
        
    def load_existing_data(self, filename: str = "complete_1_to_115_pals.csv") -> Dict[str, Dict]:
        """ê¸°ì¡´ CSV ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤"""
        existing_data = {}
        try:
            with open(filename, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                for row in reader:
                    existing_data[row['id']] = row
            print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(existing_data)}ê°œ íŒ°")
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}")
        return existing_data
    
    def get_pal_detail_url(self, pal_id: str) -> str:
        """íŒ° IDë¡œë¶€í„° ìƒì„¸ í˜ì´ì§€ URLì„ ìƒì„±í•©ë‹ˆë‹¤"""
        if 'B' in pal_id:
            base_id = pal_id.replace('B', '')
            return f"{self.base_url}/ko/Pal/{base_id}/Variant"
        else:
            return f"{self.base_url}/ko/Pal/{pal_id}"
    
    def extract_passive_skills_improved(self, soup: BeautifulSoup) -> Dict:
        """ê°œì„ ëœ Passive Skills ì¶”ì¶œ"""
        passive_data = {}
        
        try:
            passive_skills = []
            
            # ë°©ë²• 1: í…Œì´ë¸”ì—ì„œ íŒ¨ì‹œë¸Œ ìŠ¤í‚¬ ì°¾ê¸°
            for table in soup.find_all('table'):
                rows = table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    for cell in cells:
                        text = cell.get_text().strip()
                        # íŒ¨ì‹œë¸Œ ìŠ¤í‚¬ íŒ¨í„´ ë§¤ì¹­
                        if re.search(r'(LV|ë ˆë²¨|Lv)\s*\d+', text) and len(text) > 5:
                            # ì¼ë°˜ì ì¸ íŒ¨ì‹œë¸Œ ìŠ¤í‚¬ í˜•íƒœ í™•ì¸
                            if any(keyword in text for keyword in ['LV', 'ë ˆë²¨', 'Lv']):
                                skill_name = re.sub(r'\s*(LV|ë ˆë²¨|Lv)\s*\d+.*', '', text).strip()
                                if skill_name and len(skill_name) > 2:
                                    passive_skills.append(skill_name)
            
            # ë°©ë²• 2: íŒ¨ì‹œë¸Œ ì„¹ì…˜ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            for section in soup.find_all(['div', 'section', 'span']):
                text = section.get_text()
                if 'íŒ¨ì‹œë¸Œ' in text or 'passive' in text.lower():
                    # ì£¼ë³€ í…ìŠ¤íŠ¸ì—ì„œ ìŠ¤í‚¬ëª… ì°¾ê¸°
                    parent = section.parent
                    if parent:
                        surrounding_text = parent.get_text()
                        # í•œê¸€ íŒ¨ì‹œë¸Œ ìŠ¤í‚¬ëª… íŒ¨í„´
                        skill_matches = re.findall(r'([ê°€-í£\s]{3,15})\s*(?:LV|ë ˆë²¨|Lv)\s*\d+', surrounding_text)
                        for skill in skill_matches:
                            clean_skill = skill.strip()
                            if clean_skill and clean_skill not in passive_skills:
                                passive_skills.append(clean_skill)
            
            # ë°©ë²• 3: ì¼ë°˜ì ì¸ íŒ¨ì‹œë¸Œ ìŠ¤í‚¬ í‚¤ì›Œë“œë¡œ ì°¾ê¸°
            common_passive_keywords = [
                'ê³µê²©ë ¥', 'ë°©ì–´ë ¥', 'ì²´ë ¥', 'ì´ë™ì†ë„', 'ì‘ì—…ì†ë„', 'í¬íšë¥ ',
                'ë°ë¯¸ì§€', 'íš¨ìœ¨', 'ì €í•­', 'ë©´ì—­', 'ê°•í™”', 'ì¦ê°€', 'ê°ì†Œ'
            ]
            
            for element in soup.find_all(text=True):
                text = element.strip()
                if any(keyword in text for keyword in common_passive_keywords):
                    # LV íŒ¨í„´ì´ ìˆëŠ” íŒ¨ì‹œë¸Œ ìŠ¤í‚¬ ì°¾ê¸°
                    if re.search(r'LV\s*\d+', text):
                        skill_match = re.search(r'([ê°€-í£\s]{3,20})\s*LV\s*\d+', text)
                        if skill_match:
                            skill_name = skill_match.group(1).strip()
                            if skill_name and skill_name not in passive_skills:
                                passive_skills.append(skill_name)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            passive_skills = list(set(passive_skills))
            passive_skills = [skill for skill in passive_skills if len(skill.strip()) > 2]
            
            if passive_skills:
                passive_data['passiveSkills'] = " | ".join(passive_skills)
                passive_data['passiveSkills_count'] = len(passive_skills)
                print(f"  âœ… Passive Skills ë°œê²¬: {len(passive_skills)}ê°œ")
            else:
                passive_data['passiveSkills'] = ""
                passive_data['passiveSkills_count'] = 0
                print(f"  âš ï¸ Passive Skills ì—†ìŒ")
                
        except Exception as e:
            print(f"  âŒ Passive Skills ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            passive_data['passiveSkills'] = ""
            passive_data['passiveSkills_count'] = 0
            
        return passive_data
    
    def extract_active_skills_improved(self, soup: BeautifulSoup) -> Dict:
        """ê°œì„ ëœ Active Skills ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        skills_data = {}
        
        try:
            active_skills = []
            
            # ë°©ë²• 1: ìŠ¤í‚¬ í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
            for table in soup.find_all('table'):
                headers = table.find_all('th')
                if any('ìŠ¤í‚¬' in th.get_text() or 'skill' in th.get_text().lower() for th in headers):
                    rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                    
                    for row in rows:
                        cells = row.find_all('td')
                        if len(cells) >= 3:
                            skill_info = {}
                            
                            # ì²« ë²ˆì§¸ ì…€: ìŠ¤í‚¬ëª…
                            if cells[0].get_text().strip():
                                skill_info['name'] = cells[0].get_text().strip()
                            
                            # ë‚˜ë¨¸ì§€ ì…€ë“¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
                            for i, cell in enumerate(cells[1:], 1):
                                text = cell.get_text().strip()
                                
                                # ì†ì„± ì •ë³´
                                if any(element in text for element in ['í™”ì—¼', 'ë¬¼', 'í’€', 'ì–¼ìŒ', 'ë²ˆê°œ', 'ë•…', 'ì–´ë‘ ', 'ë¬´ì†ì„±']):
                                    skill_info['element'] = text
                                
                                # íŒŒì›Œ ì •ë³´
                                power_match = re.search(r'(\d+)\s*íŒŒì›Œ', text)
                                if power_match:
                                    skill_info['power'] = power_match.group(1)
                                
                                # ì¿¨íƒ€ì„ ì •ë³´
                                cooltime_match = re.search(r'(\d+)\s*ì´ˆ', text)
                                if cooltime_match:
                                    skill_info['cooltime'] = cooltime_match.group(1)
                                
                                # ë ˆë²¨ ìš”êµ¬ì‚¬í•­
                                level_match = re.search(r'Lv\.?\s*(\d+)', text)
                                if level_match:
                                    skill_info['required_level'] = level_match.group(1)
                            
                            if skill_info.get('name'):
                                active_skills.append(skill_info)
            
            # ë°©ë²• 2: ê¸°ì¡´ activeSkills í•„ë“œì—ì„œ íŒŒì‹±
            existing_skills_text = ""
            for element in soup.find_all(text=re.compile(r'.*\s*\(\s*\w+\s*ì†ì„±.*\d+\s*íŒŒì›Œ.*\d+\s*ì´ˆ\s*\)')):
                existing_skills_text += element + " | "
            
            if existing_skills_text:
                # ê¸°ì¡´ í˜•ì‹ íŒŒì‹±: "ìŠ¤í‚¬ëª…(ì†ì„±, íŒŒì›Œ, ì¿¨íƒ€ì„)"
                skill_patterns = re.findall(r'([^|]+)\(([^,]+),\s*(\d+)\s*íŒŒì›Œ,\s*(\d+)\s*ì´ˆ\)', existing_skills_text)
                for pattern in skill_patterns:
                    skill_info = {
                        'name': pattern[0].strip(),
                        'element': pattern[1].strip(),
                        'power': pattern[2],
                        'cooltime': pattern[3]
                    }
                    active_skills.append(skill_info)
            
            # ì¤‘ë³µ ì œê±°
            unique_skills = []
            seen_names = set()
            for skill in active_skills:
                if skill.get('name') and skill['name'] not in seen_names:
                    unique_skills.append(skill)
                    seen_names.add(skill['name'])
            
            if unique_skills:
                skills_data['activeSkills_detailed'] = json.dumps(unique_skills, ensure_ascii=False)
                skills_data['activeSkills_count'] = len(unique_skills)
                print(f"  âœ… Active Skills ìƒì„¸ ì •ë³´: {len(unique_skills)}ê°œ")
            else:
                skills_data['activeSkills_detailed'] = ""
                skills_data['activeSkills_count'] = 0
                print(f"  âš ï¸ Active Skills ìƒì„¸ ì •ë³´ ì—†ìŒ")
                
        except Exception as e:
            print(f"  âŒ Active Skills ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            skills_data['activeSkills_detailed'] = ""
            skills_data['activeSkills_count'] = 0
            
        return skills_data
    
    def crawl_pal_detail(self, pal_id: str) -> Dict:
        """íŒ° ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        url = self.get_pal_detail_url(pal_id)
        print(f"ğŸ” í¬ë¡¤ë§: {pal_id} - {url}")
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            pal_data = {'id': pal_id}
            
            # ìš°ì„ ìˆœìœ„ 1: Passive Skills ì¶”ì¶œ
            pal_data.update(self.extract_passive_skills_improved(soup))
            
            # ìš°ì„ ìˆœìœ„ 2: Active Skills ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            pal_data.update(self.extract_active_skills_improved(soup))
            
            # ì¶”ê°€ ì •ë³´ë“¤
            pal_data.update(self.extract_basic_info(soup))
            
            return pal_data
            
        except requests.exceptions.RequestException as e:
            print(f"  âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ {pal_id}: {str(e)}")
            return {'id': pal_id, 'error': f'network_error: {str(e)}'}
        except Exception as e:
            print(f"  âŒ íŒŒì‹± ì˜¤ë¥˜ {pal_id}: {str(e)}")
            return {'id': pal_id, 'error': f'parsing_error: {str(e)}'}
    
    def extract_basic_info(self, soup: BeautifulSoup) -> Dict:
        """ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (ì´ë¦„, ì„¤ëª… ë“±)"""
        basic_data = {}
        
        try:
            # íŒ° ì´ë¦„ ì¶”ì¶œ
            title_element = soup.find('h1') or soup.find('title')
            if title_element:
                basic_data['name_extracted'] = title_element.get_text().strip()
            
            # ì„¤ëª… ì¶”ì¶œ
            desc_element = soup.find('meta', {'name': 'description'})
            if desc_element and hasattr(desc_element, 'attrs') and 'content' in desc_element.attrs:
                basic_data['description_extracted'] = str(desc_element.attrs['content']).strip()
                
        except Exception as e:
            print(f"  âš ï¸ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        return basic_data
    
    def crawl_priority_data(self):
        """ìš°ì„ ìˆœìœ„ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        print("ğŸš€ ìš°ì„ ìˆœìœ„ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘")
        print("ìš°ì„ ìˆœìœ„: 1) passiveSkills 2) Active Skills ìƒì„¸ 3) B variants")
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_data = self.load_existing_data()
        
        results = []
        total_count = 0
        success_count = 0
        
        # 1. ê¸°ì¡´ 115ê°œ íŒ°ì˜ ëˆ„ë½ëœ ì •ë³´ ë³´ì™„
        print("\nğŸ“‹ 1ë‹¨ê³„: ê¸°ì¡´ íŒ°ë“¤ì˜ ëˆ„ë½ëœ ì •ë³´ ë³´ì™„")
        for pal_id in range(1, 116):
            pal_data = self.crawl_pal_detail(str(pal_id))
            results.append(pal_data)
            total_count += 1
            
            if 'error' not in pal_data:
                success_count += 1
            
            time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        # 2. B variants ëŒ€ëŸ‰ ì¶”ê°€
        print(f"\nğŸ“‹ 2ë‹¨ê³„: B variants í¬ë¡¤ë§ ({len(self.known_b_variants)}ê°œ)")
        b_success = 0
        for variant_id in self.known_b_variants:
            print(f"  ğŸ”„ {variant_id} ì²˜ë¦¬ ì¤‘...")
            pal_data = self.crawl_pal_detail(variant_id)
            results.append(pal_data)
            total_count += 1
            
            if 'error' not in pal_data:
                success_count += 1
                b_success += 1
            
            time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        print(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼:")
        print(f"  ì´ ì‹œë„: {total_count}ê°œ")
        print(f"  ì„±ê³µ: {success_count}ê°œ ({success_count/total_count*100:.1f}%)")
        print(f"  B variants ì„±ê³µ: {b_success}ê°œ / {len(self.known_b_variants)}ê°œ")
        
        # ê²°ê³¼ ì €ì¥
        self.save_enhanced_data(existing_data, results)
        
        return results
    
    def save_enhanced_data(self, existing_data: Dict, new_data: List[Dict]):
        """í–¥ìƒëœ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤"""
        print("\nğŸ’¾ ë°ì´í„° ë³‘í•© ë° ì €ì¥ ì¤‘...")
        
        # ìƒˆ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        new_data_dict = {item['id']: item for item in new_data if 'id' in item}
        
        merged_data = []
        
        # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
        for pal_id, existing_row in existing_data.items():
            updated_row = existing_row.copy()
            
            if pal_id in new_data_dict:
                new_row = new_data_dict[pal_id]
                
                # ìƒˆë¡œìš´ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸ (ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                for key, value in new_row.items():
                    if value and value != "":
                        updated_row[key] = value
            
            merged_data.append(updated_row)
        
        # ìƒˆë¡œìš´ B variants ì¶”ê°€
        existing_ids = set(existing_data.keys())
        for pal_id, new_row in new_data_dict.items():
            if pal_id not in existing_ids and 'error' not in new_row:
                # ê¸°ë³¸ êµ¬ì¡°ë¡œ ìƒˆ í–‰ ìƒì„±
                new_pal_row = {
                    'id': pal_id,
                    'name_kor': new_row.get('name_extracted', ''),
                    'description_kor': new_row.get('description_extracted', ''),
                    'passiveSkills': new_row.get('passiveSkills', ''),
                    'passiveSkills_count': new_row.get('passiveSkills_count', 0),
                    'activeSkills_detailed': new_row.get('activeSkills_detailed', ''),
                    'activeSkills_count': new_row.get('activeSkills_count', 0)
                }
                merged_data.append(new_pal_row)
        
        # CSVë¡œ ì €ì¥
        output_filename = 'improved_complete_pals_data.csv'
        
        if merged_data:
            # ëª¨ë“  ì»¬ëŸ¼ ìˆ˜ì§‘
            all_columns = set()
            for row in merged_data:
                all_columns.update(row.keys())
            
            with open(output_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=sorted(all_columns))
                writer.writeheader()
                writer.writerows(merged_data)
            
            print(f"âœ… í–¥ìƒëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_filename}")
            print(f"  ì´ íŒ° ìˆ˜: {len(merged_data)}ê°œ")
            
            # í†µê³„ ì¶œë ¥
            passive_filled = sum(1 for row in merged_data if row.get('passiveSkills', ''))
            active_detailed = sum(1 for row in merged_data if row.get('activeSkills_detailed', ''))
            b_variants = sum(1 for row in merged_data if 'B' in str(row.get('id', '')))
            
            print(f"  Passive Skills ë³´ìœ : {passive_filled}ê°œ")
            print(f"  Active Skills ìƒì„¸: {active_detailed}ê°œ") 
            print(f"  B variants: {b_variants}ê°œ")
        else:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ê°œì„ ëœ íŒ°ì›”ë“œ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("ëª©í‘œ: passiveSkills + Active Skills ìƒì„¸ + B variants ëŒ€ëŸ‰ ì¶”ê°€")
    
    crawler = ImprovedPalCrawler()
    
    # ìš°ì„ ìˆœìœ„ í¬ë¡¤ë§ ì‹¤í–‰
    results = crawler.crawl_priority_data()
    
    print(f"\nâœ¨ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(results)}ê°œ íŒ° ì²˜ë¦¬")

if __name__ == "__main__":
    main() 