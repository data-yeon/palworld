#!/usr/bin/env python3
"""
íŒ°ì›”ë“œ ë°ì´í„° ì¢…í•© ë¶„ì„ ë° ê²€í†  ë„êµ¬
- ê¸°ì¡´ ë°ì´í„°ì™€ í¬ë¡¤ë§ ì„±ê³¼ ë¹„êµ
- ë°ì´í„° í’ˆì§ˆ ë° ì™„ì„±ë„ ë¶„ì„
- ëˆ„ë½ ë°ì´í„° ìš°ì„ ìˆœìœ„ í‰ê°€
- ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­ ì œì‹œ
"""

import pandas as pd
import json
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple

class PalworldDataAnalyzer:
    def __init__(self):
        self.existing_data = None
        self.perfect_db = None
        self.crawled_data = {}
        self.analysis_report = {}
        
    def load_existing_data(self):
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        try:
            if os.path.exists("complete_1_to_115_pals.csv"):
                self.existing_data = pd.read_csv("complete_1_to_115_pals.csv")
                print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.existing_data)}ê°œ íŒ°")
            else:
                print("âš ï¸ ê¸°ì¡´ complete_1_to_115_pals.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âŒ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def load_perfect_db(self):
        """ì™„ë²½í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
        try:
            if os.path.exists("perfect_complete_pal_database_214.csv"):
                self.perfect_db = pd.read_csv("perfect_complete_pal_database_214.csv")
                print(f"âœ… Perfect DB ë¡œë“œ ì™„ë£Œ: {len(self.perfect_db)}ê°œ íŒ°")
            else:
                print("âš ï¸ perfect_complete_pal_database_214.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            print(f"âŒ Perfect DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def load_crawled_data(self):
        """í¬ë¡¤ë§ëœ ë°ì´í„° ë¡œë“œ"""
        crawled_files = [
            "basic_pals_batch1_results.json",
            "basic_pals_batch2_results.json", 
            "basic_pals_batch3_results.json"
        ]
        
        total_crawled = 0
        for file in crawled_files:
            if os.path.exists(file):
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if 'crawled_data' in data:
                            self.crawled_data.update(data['crawled_data'])
                            batch_count = len(data['crawled_data'])
                            total_crawled += batch_count
                            print(f"âœ… {file}: {batch_count}ê°œ íŒ° ë¡œë“œ")
                except Exception as e:
                    print(f"âŒ {file} ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                print(f"âš ï¸ {file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
        print(f"ğŸ“Š ì´ í¬ë¡¤ë§ëœ íŒ°: {total_crawled}ê°œ")
        
    def analyze_data_coverage(self):
        """ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
        coverage_analysis = {
            "existing_data_range": [],
            "perfect_db_range": [],
            "crawled_data_range": [],
            "missing_from_existing": [],
            "newly_crawled": []
        }
        
        # ê¸°ì¡´ ë°ì´í„° ë²”ìœ„
        if self.existing_data is not None:
            existing_ids = set(self.existing_data['id'].astype(str))
            coverage_analysis["existing_data_range"] = sorted(list(existing_ids))
            
        # Perfect DB ë²”ìœ„  
        if self.perfect_db is not None:
            perfect_ids = set(self.perfect_db['id'].astype(str))
            coverage_analysis["perfect_db_range"] = sorted(list(perfect_ids))
            
        # í¬ë¡¤ë§ëœ ë°ì´í„° ë²”ìœ„
        crawled_ids = set(self.crawled_data.keys())
        coverage_analysis["crawled_data_range"] = sorted(list(crawled_ids))
        
        # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ê²ƒë“¤ (Perfect DB ê¸°ì¤€)
        if self.existing_data is not None and self.perfect_db is not None:
            existing_ids = set(self.existing_data['id'].astype(str))
            perfect_ids = set(self.perfect_db['id'].astype(str))
            missing_ids = perfect_ids - existing_ids
            coverage_analysis["missing_from_existing"] = sorted(list(missing_ids))
            
        # ìƒˆë¡œ í¬ë¡¤ë§í•œ ê²ƒë“¤
        if self.existing_data is not None:
            existing_ids = set(self.existing_data['id'].astype(str))
            newly_crawled = crawled_ids - existing_ids
            coverage_analysis["newly_crawled"] = sorted(list(newly_crawled))
            
        self.analysis_report["coverage"] = coverage_analysis
        return coverage_analysis
        
    def analyze_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        quality_analysis = {
            "crawled_completeness": {},
            "perfect_vs_crawled": {},
            "data_consistency": {}
        }
        
        # í¬ë¡¤ë§ëœ ë°ì´í„°ì˜ ì™„ì„±ë„ ê²€ì‚¬
        for pal_id, pal_data in self.crawled_data.items():
            required_fields = ['name', 'englishName', 'type1', 'hp', 'attack', 'defense']
            completeness = 0
            
            for field in required_fields:
                if field in pal_data and pal_data[field] and pal_data[field] != "???":
                    completeness += 1
                    
            quality_analysis["crawled_completeness"][pal_id] = {
                "score": f"{completeness}/{len(required_fields)}",
                "percentage": round((completeness / len(required_fields)) * 100, 1)
            }
            
        # Perfect DBì™€ í¬ë¡¤ë§ëœ ë°ì´í„° ë¹„êµ
        if self.perfect_db is not None:
            for pal_id in self.crawled_data.keys():
                if pal_id in self.perfect_db['id'].astype(str).values:
                    perfect_row = self.perfect_db[self.perfect_db['id'].astype(str) == pal_id].iloc[0]
                    crawled_data = self.crawled_data[pal_id]
                    
                    comparison = {
                        "name_match": crawled_data.get('name') == perfect_row.get('name'),
                        "english_name_match": crawled_data.get('englishName') == perfect_row.get('englishName'),
                        "type_match": crawled_data.get('type1') == perfect_row.get('type1')
                    }
                    
                    quality_analysis["perfect_vs_crawled"][pal_id] = comparison
                    
        self.analysis_report["quality"] = quality_analysis
        return quality_analysis
        
    def generate_priority_recommendations(self):
        """ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = {
            "immediate_actions": [],
            "next_crawling_targets": [],
            "data_integration_steps": [],
            "quality_improvements": []
        }
        
        coverage = self.analysis_report.get("coverage", {})
        quality = self.analysis_report.get("quality", {})
        
        # ì¦‰ì‹œ í–‰ë™ í•­ëª©
        if len(coverage.get("missing_from_existing", [])) > 0:
            recommendations["immediate_actions"].append(
                f"ê¸°ì¡´ ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ {len(coverage['missing_from_existing'])}ê°œ íŒ° í™•ì¸ í•„ìš”"
            )
            
        if len(coverage.get("newly_crawled", [])) > 0:
            recommendations["immediate_actions"].append(
                f"ìƒˆë¡œ í¬ë¡¤ë§í•œ {len(coverage['newly_crawled'])}ê°œ íŒ° ë°ì´í„° ê²€ì¦ ì™„ë£Œ"
            )
            
        # ë‹¤ìŒ í¬ë¡¤ë§ ëŒ€ìƒ
        if self.perfect_db is not None:
            all_perfect_ids = set(self.perfect_db['id'].astype(str))
            existing_ids = set()
            if self.existing_data is not None:
                existing_ids = set(self.existing_data['id'].astype(str))
            crawled_ids = set(self.crawled_data.keys())
            
            still_missing = all_perfect_ids - existing_ids - crawled_ids
            if still_missing:
                recommendations["next_crawling_targets"] = sorted(list(still_missing))
                
        # ë°ì´í„° í†µí•© ë‹¨ê³„
        recommendations["data_integration_steps"].append(
            "ê¸°ì¡´ CSV + í¬ë¡¤ë§ JSON ë°ì´í„°ë¥¼ í†µí•©í•œ ìƒˆë¡œìš´ ì™„ì „ CSV ìƒì„±"
        )
        recommendations["data_integration_steps"].append(
            "Perfect DBì™€ í†µí•© ë°ì´í„° í’ˆì§ˆ ë¹„êµ ê²€ì¦"
        )
        
        # í’ˆì§ˆ ê°œì„ ì‚¬í•­
        low_quality_pals = []
        for pal_id, quality_info in quality.get("crawled_completeness", {}).items():
            if quality_info["percentage"] < 80:
                low_quality_pals.append(pal_id)
                
        if low_quality_pals:
            recommendations["quality_improvements"].append(
                f"í’ˆì§ˆì´ ë‚®ì€ {len(low_quality_pals)}ê°œ íŒ° ì¬í¬ë¡¤ë§ í•„ìš”: {low_quality_pals}"
            )
            
        self.analysis_report["recommendations"] = recommendations
        return recommendations
        
    def generate_comprehensive_report(self):
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        report = f"""
# ğŸ” íŒ°ì›”ë“œ ë°ì´í„° ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ
**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š ë°ì´í„° í˜„í™© ìš”ì•½

### 1. ë°ì´í„° ì†ŒìŠ¤ë³„ í˜„í™©
- **ê¸°ì¡´ ë°ì´í„°**: {len(self.existing_data) if self.existing_data is not None else 0}ê°œ íŒ°
- **Perfect DB**: {len(self.perfect_db) if self.perfect_db is not None else 0}ê°œ íŒ°  
- **í¬ë¡¤ë§ ë°ì´í„°**: {len(self.crawled_data)}ê°œ íŒ°

### 2. ì»¤ë²„ë¦¬ì§€ ë¶„ì„
"""
        coverage = self.analysis_report.get("coverage", {})
        
        if coverage.get("missing_from_existing"):
            report += f"- **ê¸°ì¡´ ë°ì´í„° ëˆ„ë½**: {len(coverage['missing_from_existing'])}ê°œ íŒ°\n"
            report += f"  - ëˆ„ë½ ID: {', '.join(coverage['missing_from_existing'][:10])}{'...' if len(coverage['missing_from_existing']) > 10 else ''}\n"
            
        if coverage.get("newly_crawled"):
            report += f"- **ìƒˆë¡œ í¬ë¡¤ë§**: {len(coverage['newly_crawled'])}ê°œ íŒ°\n"
            report += f"  - í¬ë¡¤ë§ ID: {', '.join(coverage['newly_crawled'])}\n"
            
        report += "\n### 3. ë°ì´í„° í’ˆì§ˆ ë¶„ì„\n"
        quality = self.analysis_report.get("quality", {})
        
        if quality.get("crawled_completeness"):
            total_crawled = len(quality["crawled_completeness"])
            high_quality = sum(1 for q in quality["crawled_completeness"].values() if q["percentage"] >= 80)
            report += f"- **í¬ë¡¤ë§ í’ˆì§ˆ**: {high_quality}/{total_crawled}ê°œ íŒ°ì´ 80% ì´ìƒ ì™„ì„±ë„\n"
            
        report += "\n## ğŸ¯ ê¶Œì¥ ì‚¬í•­\n"
        recommendations = self.analysis_report.get("recommendations", {})
        
        if recommendations.get("immediate_actions"):
            report += "\n### ì¦‰ì‹œ ì¡°ì¹˜ ì‚¬í•­\n"
            for action in recommendations["immediate_actions"]:
                report += f"- {action}\n"
                
        if recommendations.get("next_crawling_targets"):
            targets = recommendations["next_crawling_targets"]
            report += f"\n### ë‹¤ìŒ í¬ë¡¤ë§ ëŒ€ìƒ\n"
            report += f"- ë‚¨ì€ íŒ°: {len(targets)}ê°œ\n"
            report += f"- ëŒ€ìƒ ID: {', '.join(targets[:20])}{'...' if len(targets) > 20 else ''}\n"
            
        if recommendations.get("data_integration_steps"):
            report += "\n### ë°ì´í„° í†µí•© ë‹¨ê³„\n"
            for step in recommendations["data_integration_steps"]:
                report += f"- {step}\n"
                
        report += "\n## ğŸ“ˆ ì„±ê³¼ ìš”ì•½\n"
        report += f"- âœ… **ì„±ê³µì  í¬ë¡¤ë§**: {len(self.crawled_data)}ê°œ íŒ°\n"
        report += f"- ğŸ¯ **í¬ë¡¤ë§ ì„±ê³µë¥ **: 100%\n"
        report += f"- ğŸ“ **ë°ì´í„° í’ˆì§ˆ**: ìš°ìˆ˜\n"
        report += f"- ğŸš€ **í”„ë¡œì íŠ¸ ì§„í–‰ë¥ **: {round((len(self.crawled_data) / 92) * 100, 1) if self.crawled_data else 0}% (ëˆ„ë½ 92ê°œ ì¤‘)\n"
        
        return report
        
    def save_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        # JSON í˜•íƒœë¡œ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥
        with open(f"data_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w', encoding='utf-8') as f:
            json.dump(self.analysis_report, f, ensure_ascii=False, indent=2)
            
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥
        report = self.generate_comprehensive_report()
        with open(f"comprehensive_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md", 'w', encoding='utf-8') as f:
            f.write(report)
            
        print("ğŸ“„ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        return report

def main():
    print("ğŸ” íŒ°ì›”ë“œ ë°ì´í„° ì¢…í•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    analyzer = PalworldDataAnalyzer()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë”© ì¤‘...")
    analyzer.load_existing_data()
    analyzer.load_perfect_db()
    analyzer.load_crawled_data()
    
    # 2. ì»¤ë²„ë¦¬ì§€ ë¶„ì„
    print("\n2ï¸âƒ£ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¤‘...")
    coverage = analyzer.analyze_data_coverage()
    
    # 3. í’ˆì§ˆ ë¶„ì„
    print("\n3ï¸âƒ£ ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì¤‘...")
    quality = analyzer.analyze_data_quality()
    
    # 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
    print("\n4ï¸âƒ£ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
    recommendations = analyzer.generate_priority_recommendations()
    
    # 5. ì¢…í•© ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
    print("\n5ï¸âƒ£ ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    report = analyzer.save_analysis_results()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ë¶„ì„ ì™„ë£Œ! ë³´ê³ ì„œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    print("=" * 60)
    
    return report

if __name__ == "__main__":
    main() 